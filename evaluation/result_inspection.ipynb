{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc791ba",
   "metadata": {},
   "source": [
    "# Result Inspection\n",
    "Some utility functions for inspecting JSON results of benchmark runs stored in `data\\evaluation\\50000006`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f783b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import json\n",
    "from typing_extensions import List\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "script_dir = os.path.abspath('')\n",
    "eval_dir = os.path.join(script_dir, \"../data/evaluation/50000006\")\n",
    "\n",
    "# We create a pandas dataframe that contains the full data for every experiment,\n",
    "# and add a column with the filename (without .json suffix)\n",
    "all_dfs = []\n",
    "for filename in os.listdir(eval_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(eval_dir, filename), 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            if not content:  # Check if the file is empty\n",
    "                print(f\"Skipping empty file: {filename}\")\n",
    "                continue\n",
    "            try:\n",
    "                json_obj = json.loads(content)\n",
    "                data_list = json_obj[\"data\"]\n",
    "                df_tmp = pd.DataFrame(data_list)\n",
    "                df_tmp['filename'] = filename.split('.')[0]\n",
    "\n",
    "                # Add each meta field as a column with the same value for all rows\n",
    "                meta_info = json_obj.get(\"meta\")  # Get metadata dict\n",
    "                for key, value in meta_info.items():\n",
    "                    df_tmp[key] = value\n",
    "                # Unpack the 'results' column into separate columns\n",
    "                results_expanded = df_tmp['results'].apply(pd.Series)\n",
    "                df_tmp = pd.concat([df_tmp.drop(columns=['results']), results_expanded], axis=1)\n",
    "                # Ensure 'correct' column is always float (0.0/1.0), even if it was bool or object\n",
    "                if 'correct' in df_tmp.columns:\n",
    "                    df_tmp['correct'] = df_tmp['correct'].astype(float)\n",
    "                # Map all other boolean columns to 0/1\n",
    "                for col in df_tmp.columns:\n",
    "                    if df_tmp[col].dtype == 'bool':\n",
    "                        df_tmp[col] = df_tmp[col].astype(float)\n",
    "                all_dfs.append(df_tmp)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON file: {filename}, error: {e}\")\n",
    "                continue\n",
    "\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "df.head()\n",
    "# Print all columns in the DataFrame\n",
    "print(\"Columns in the DataFrame:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7eaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_runs(df, run_type, n=1, required_keywords=[], retrieval_llm=None, coding_llm=None, retrieval_only=False, analysis_only=False, iterative_only=False):\n",
    "    runs = df[df['filename'].str.startswith(run_type)]['filename'].unique()\n",
    "\n",
    "    if retrieval_llm:\n",
    "        runs = [run for run in runs if df[df['filename'] == run]['retrieval_llm'].iloc[0] == retrieval_llm]\n",
    "\n",
    "    if coding_llm:\n",
    "        runs = [run for run in runs if df[df['filename'] == run]['coding_llm'].iloc[0] == coding_llm]\n",
    "\n",
    "    if retrieval_only:\n",
    "        runs = [run for run in runs if df[df['filename'] == run]['only_retrieval'].iloc[0] == 1.0]\n",
    "\n",
    "    if analysis_only:\n",
    "        runs = [run for run in runs if df[df['filename'] == run]['only_analysis'].iloc[0] == 1.0]\n",
    "\n",
    "    if iterative_only:\n",
    "        runs = [run for run in runs if df[df['filename'] == run]['analyzer'].iloc[0] == 'iterative_local']\n",
    "\n",
    "    if required_keywords:\n",
    "        runs = [run for run in runs if all(keyword in run for keyword in required_keywords)]\n",
    "\n",
    "    timestamps = [run.split(\"_\")[-2] + \"_\" + run.split(\"_\")[-1] for run in runs]\n",
    "    sorted_runs = [run for _, run in sorted(zip(timestamps, runs))]\n",
    "    return sorted_runs[-n:] if len(sorted_runs) >= n else sorted_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_comparison_retrieval(df: pd.DataFrame, experiment_names: List[str], columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a comparison table for the specified columns across multiple experiments.\n",
    "    \"\"\"\n",
    "    comparison_data = {}\n",
    "    \n",
    "    for exp in experiment_names:\n",
    "        df_exp = df[df['filename'] == exp]\n",
    "        comparison_data[exp] = {}\n",
    "\n",
    "        for col in columns:\n",
    "            if col in ['recall', 'precision', 'validation_accuracy']:\n",
    "                comparison_data[exp][col] = round(df_exp[col].mean() * 100, 1)\n",
    "            else:\n",
    "                if col == \"retrieval_latency\":\n",
    "                    decimals = 1\n",
    "                elif col == \"retrieval_tokens\":\n",
    "                    decimals = 0\n",
    "                elif col == \"retrieval_cost_usd\":\n",
    "                    decimals = 3\n",
    "                else:\n",
    "                    decimals = 1\n",
    "\n",
    "                comparison_data[exp][col] = round(df_exp[col].median(), decimals)\n",
    "                # Add interquartile range as column\n",
    "                q1 = df_exp[col].quantile(0.25)\n",
    "                q3 = df_exp[col].quantile(0.75)\n",
    "                comparison_data[exp][f'{col}_q1'] = round(q1, decimals)\n",
    "                comparison_data[exp][f'{col}_q3'] = round(q3, decimals)\n",
    "\n",
    "                # If decimals are 0, cast to int\n",
    "                if decimals == 0:\n",
    "                    comparison_data[exp][col] = int(comparison_data[exp][col])\n",
    "                    comparison_data[exp][f'{col}_q1'] = int(comparison_data[exp][f'{col}_q1'])\n",
    "                    comparison_data[exp][f'{col}_q3'] = int(comparison_data[exp][f'{col}_q3'])\n",
    "        # Add LLM information\n",
    "        comparison_data[exp]['retrieval_llm'] = df_exp['retrieval_llm'].iloc[0]\n",
    "        comparison_data[exp]['retrieval_nr_searches'] = df_exp['retrieval_nr_searches'].mean()\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data).T\n",
    "    comparison_df.index.name = 'Experiment'\n",
    "    return comparison_df\n",
    "\n",
    "flat_retrieval_runs = ['agentic_retrieval_only_retrieval_gpt4o_retrieval_20250710_114251', 'agentic_retrieval_only_retrieval_41_20250715_113532', 'agentic_retrieval_only_retrieval_gpt41mini_20250715_113056',\n",
    "                       'agentic_retrieval_only_retrieval_o1_parallel_20250710_135131', 'agentic_retrieval_only_retrieval_20250823_153832',\n",
    "                       'agentic_retrieval_only_retrieval_25flash_20250715_113302', 'agentic_retrieval_only_retrieval_gemini25pro_20250710_134756', 'agentic_retrieval_only_retrieval_mistral_retrieval_fixed_20250710_134614',\n",
    "                       'agentic_retrieval_only_retrieval_20250814_111731', 'agentic_retrieval_only_retrieval_20250814_113251']\n",
    "\n",
    "table_comparison_retrieval(df, flat_retrieval_runs, ['recall', 'precision', 'validation_accuracy', 'retrieval_latency', 'retrieval_tokens', 'retrieval_cost_usd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_result_df = table_comparison_retrieval(df, flat_retrieval_runs, ['recall', 'precision', 'validation_accuracy', 'retrieval_latency', 'retrieval_tokens', 'retrieval_cost_usd'])\n",
    "retrieval_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_strategy_and_approach_runs = ['verified_retrieval_bm25_only_retrieval_20250806_143025', 'verified_retrieval_only_retrieval_20250806_133834', 'verified_retrieval_hybrid_only_retrieval_20250806_145204',\n",
    "                                     'agentic_retrieval_bm25_only_retrieval_20250806_162133', 'agentic_retrieval_only_retrieval_41_20250715_113532', 'agentic_retrieval_hybrid_only_retrieval_20250806_152631']\n",
    "\n",
    "table_comparison_retrieval(df, search_strategy_and_approach_runs, ['recall', 'precision', 'validation_accuracy', 'retrieval_latency', 'retrieval_tokens', 'retrieval_cost_usd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a62132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_comparison_analysis(df: pd.DataFrame, experiment_names: List[str], columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a comparison table for the specified columns across multiple experiments.\n",
    "    \"\"\"\n",
    "    comparison_data = {}\n",
    "    \n",
    "    for exp in experiment_names:\n",
    "        df_exp = df[df['filename'] == exp]\n",
    "        comparison_data[exp] = {}\n",
    "\n",
    "        for col in columns:\n",
    "            if col == 'correct':\n",
    "                # Only consider solvable tasks for correctness\n",
    "                solvable_tasks = df_exp[df_exp['reference_titles'].apply(lambda x: len(x) != 0)]\n",
    "                # Count number of correct and incorrect and print results\n",
    "\n",
    "                comparison_data[exp][col] = solvable_tasks[col].mean() * 100\n",
    "            else:\n",
    "                if col == \"coding_latency\":\n",
    "                    decimals = 1\n",
    "                elif col == \"coding_tokens\":\n",
    "                    decimals = 0\n",
    "                elif col == \"coding_cost_usd\":\n",
    "                    decimals = 3\n",
    "                else:\n",
    "                    decimals = 1\n",
    "\n",
    "                comparison_data[exp][col] = (df_exp[col].median())\n",
    "                # Add interquartile range as column\n",
    "                q1 = df_exp[col].quantile(0.25)\n",
    "                q3 = df_exp[col].quantile(0.75)\n",
    "                comparison_data[exp][f'{col}_q1'] = (q1)\n",
    "                comparison_data[exp][f'{col}_q3'] = (q3)\n",
    "\n",
    "                # If decimals are 0, cast to int\n",
    "                if decimals == 0:\n",
    "                    comparison_data[exp][col] = int(comparison_data[exp][col])\n",
    "                    comparison_data[exp][f'{col}_q1'] = int(comparison_data[exp][f'{col}_q1'])\n",
    "                    comparison_data[exp][f'{col}_q3'] = int(comparison_data[exp][f'{col}_q3'])\n",
    "        # Add LLM information\n",
    "        comparison_data[exp]['coding_llm'] = df_exp['coding_llm'].iloc[0]\n",
    "        comparison_data[exp]['analyzer'] = df_exp['analyzer'].iloc[0]\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data).T\n",
    "    comparison_df.index.name = 'Experiment'\n",
    "    return comparison_df\n",
    "\n",
    "analysis_llms = ['mistral-large', 'gpt-4o', 'gpt-o1', 'gpt-4.1', 'gemini-2.5-flash', 'gpt-4.1-mini', 'gemini-2.5-pro',\n",
    "                 'gpt-5', 'meta-llama/llama-4-maverick', 'openai/gpt-oss-120b', 'mistral-codestral']\n",
    "analysis_runs = [get_latest_runs(df, \"\", n=5, coding_llm=llm, analysis_only=True, iterative_only=False) for llm in analysis_llms]\n",
    "flat_analysis_runs = [run for sublist in analysis_runs for run in sublist]\n",
    "\n",
    "table_comparison_analysis(df, flat_analysis_runs, ['correct', 'coding_latency', 'coding_tokens', 'coding_cost_usd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d862e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_pass_vs_iterative_runs = ['agentic_retrieval_only_analysis_20250812_104751', 'agentic_retrieval_only_analysis_gpt41_iterv2_20250721_172254']\n",
    "table_comparison_analysis(df, single_pass_vs_iterative_runs, ['correct', 'coding_latency', 'coding_tokens', 'coding_cost_usd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d39f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_comparison_runs_analysis = [\n",
    "    'agentic_retrieval_only_analysis_iter4o_20250812_140604',\n",
    "    'agentic_retrieval_only_analysis_gpt41_iterv2_20250721_172254', \n",
    "    'agentic_retrieval_only_analysis_gpt41mini_20250715_135007', \n",
    "    'agentic_retrieval_only_analysis__gpt-o1_simple_local_v2_20250813_010136', \n",
    "    'agentic_retrieval_only_analysis_20250823_171529', \n",
    "    'agentic_retrieval_only_analysis_gemini25flash_20250714_141142', \n",
    "    'agentic_retrieval_only_analysis_20250813_120608', \n",
    "    'agentic_retrieval_only_analysis_20250813_155235',\n",
    "    'agentic_retrieval_only_analysis__gpt-oss-120b_simple_local_v2_20250813_120323', \n",
    "    'agentic_retrieval_only_analysis__llama-4-maverick_simple_local_v2_20250813_122444'\n",
    "]\n",
    "\n",
    "table_comparison_analysis(df, llm_comparison_runs_analysis, ['correct', 'coding_latency', 'coding_tokens', 'coding_cost_usd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column latency and token, replacing NaN with 0\n",
    "df['total_latency'] = df['retrieval_latency'].fillna(0) + df['coding_latency'].fillna(0)\n",
    "df['total_tokens'] = df['retrieval_tokens'].fillna(0) + df['coding_tokens'].fillna(0)\n",
    "df['total_cost'] = df['retrieval_cost_usd'].fillna(0) + df['coding_cost_usd'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_comparison_endtoend(df: pd.DataFrame, experiment_names: List[str], columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a comparison table for the specified columns across multiple experiments.\n",
    "    \"\"\"\n",
    "    comparison_data = {}\n",
    "    \n",
    "    for exp in experiment_names:\n",
    "        df_exp = df[df['filename'] == exp]\n",
    "        comparison_data[exp] = {}\n",
    "\n",
    "        for col in columns:\n",
    "            if col in ['recall', 'precision', 'validation_accuracy']:\n",
    "                comparison_data[exp][col] = df_exp[col].mean() * 100\n",
    "            elif col == 'correct':\n",
    "                # Only consider solvable tasks for correctness\n",
    "                solvable_tasks = df_exp[df_exp['reference_titles'].apply(lambda x: len(x) != 0)]\n",
    "                comparison_data[exp][col] = solvable_tasks[col].mean() * 100\n",
    "            else:\n",
    "                comparison_data[exp][col] = (df_exp[col].median())\n",
    "        # Add LLM information\n",
    "        comparison_data[exp]['retrieval_llm'] = df_exp['retrieval_llm'].iloc[0]\n",
    "        comparison_data[exp]['retrieval_nr_searches'] = df_exp['retrieval_nr_searches'].mean()\n",
    "        comparison_data[exp]['coding_llm'] = df_exp['coding_llm'].iloc[0]\n",
    "\n",
    "        # Add total median latency, token and cost by computing sums of retrieval latency and coding latency, then taking median\n",
    "        comparison_data[exp]['total_latency'] = round(df_exp['total_latency'].median(), 1)\n",
    "        comparison_data[exp]['total_latency_q1'] = round(df_exp['total_latency'].quantile(0.25), 1)\n",
    "        comparison_data[exp]['total_latency_q3'] = round(df_exp['total_latency'].quantile(0.75), 1)\n",
    "        comparison_data[exp]['total_tokens'] = int(df_exp['total_tokens'].median())\n",
    "        comparison_data[exp]['total_tokens_q1'] = int(df_exp['total_tokens'].quantile(0.25))\n",
    "        comparison_data[exp]['total_tokens_q3'] = int(df_exp['total_tokens'].quantile(0.75))\n",
    "        comparison_data[exp]['total_cost'] = round(df_exp['total_cost'].median(), 3)\n",
    "        comparison_data[exp]['total_cost_q1'] = round(df_exp['total_cost'].quantile(0.25), 3)\n",
    "        comparison_data[exp]['total_cost_q3'] = round(df_exp['total_cost'].quantile(0.75), 3)\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data).T\n",
    "    comparison_df.index.name = 'Experiment'\n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "stability_runs = [\n",
    "                  #'agentic_retrieval_stability__20250815_053614',\n",
    "                  'agentic_retrieval_stability2_rephrased_0_20250815_150752',\n",
    "                  'agentic_retrieval_stability2_rephrased_1_20250815_161922',\n",
    "                  'agentic_retrieval_stability2_rephrased_2_20250815_172506',\n",
    "                  'agentic_retrieval_stability2_rephrased_3_20250816_112856',\n",
    "                  'agentic_retrieval_stability2_rephrased_4_20250816_123217',\n",
    "                  'agentic_retrieval_stability__20250814_194413'\n",
    "                  ]\n",
    "\n",
    "language_runs = ['agentic_retrieval_stability_translated_en_20250815_020058', 'agentic_retrieval_stability_translated_fr_20250815_025910', 'agentic_retrieval_stability_translated_it_20250815_043557']\n",
    "\n",
    "print(\"Stability comparison\")\n",
    "display(table_comparison_endtoend(df, stability_runs, ['correct', 'recall', 'precision', 'validation_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07de10",
   "metadata": {},
   "source": [
    "### Inspect answers in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, HTML\n",
    "\n",
    "def display_answer_comparison(df, experiment_name, incorrect_only=True, id_start=0):\n",
    "    display(HTML(f\"<h2>Answer comparison for {experiment_name}</h2>\"))\n",
    "\n",
    "    df_exp = df[df['filename'] == experiment_name]\n",
    "    #predicted_not_nan = df_exp[df_exp['predicted_answer'].notna()]\n",
    "    # keep only where relevant datasets are present\n",
    "    predicted_not_nan = df_exp[df_exp['reference_titles'].apply(lambda x: len(x) != 0)]\n",
    "\n",
    "    if incorrect_only:\n",
    "        predicted_not_nan = predicted_not_nan[predicted_not_nan['correct'] != 1.0]\n",
    "        print(f\"Displaying {len(predicted_not_nan)} incorrect predictions for {experiment_name}.\")\n",
    "    else:\n",
    "        print(f\"Displaying {len(predicted_not_nan)} predictions for {experiment_name}.\")\n",
    "\n",
    "    for _, row in predicted_not_nan.iterrows():\n",
    "\n",
    "        if int(row['id'].split(\"_\")[0]) < id_start:\n",
    "            continue\n",
    "\n",
    "        display(Markdown(f\"**Question:**\\n\\n{row['question']}\"))\n",
    "        color = 'red'\n",
    "        if row['precision'] == 1.0 and row['recall'] == 1.0:\n",
    "            color = 'green'\n",
    "        elif row['precision'] != 1.0 and row['recall'] == 1.0:\n",
    "            color = 'orange'\n",
    "        display(HTML(f'<span style=\"color: {color}\">Reference Titles: {row['reference_titles']} | Predicted Titles: {row['predicted_titles']}</span>'))\n",
    "\n",
    "        if pd.notna(row['reference_answer']):\n",
    "            display(Markdown(f\"**Reference Answer:**\\n\\n{row['reference_answer']}\"))\n",
    "        if pd.notna(row['predicted_answer']):\n",
    "            display(Markdown(f\"**Predicted Answer:**\\n\\n{row['predicted_answer']}\"))\n",
    "\n",
    "        if pd.notna(row[\"reference_answer\"]) and pd.notna(row[\"predicted_answer\"]):\n",
    "            if row[\"correct\"] == 1.0:\n",
    "                display(HTML('<span style=\"color: green; font-weight: bold;\">LLM Judge: Correct prediction</span>'))\n",
    "            else:\n",
    "                display(HTML('<span style=\"color: red; font-weight: bold;\">LLM Judge: Incorrect prediction</span>'))\n",
    "\n",
    "            # if str(row[\"reference_answer\"]).strip() in str(row[\"predicted_answer\"]):\n",
    "            #     display(HTML('<span style=\"color: green;\">Substring Match: Correct prediction</span>'))\n",
    "            # else:\n",
    "            #     display(HTML('<span style=\"color: red;\">Substring Match: Incorrect prediction</span>'))\n",
    "        display(HTML('<hr style=\"margin: 0px 0;\">'))\n",
    "\n",
    "for run in stability_runs:\n",
    "    display_answer_comparison(df, run, incorrect_only=True)\n",
    "    display(HTML('<hr style=\"margin: 0px 0;\">'))\n",
    "    display(HTML('<hr style=\"margin: 0px 0;\">'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
